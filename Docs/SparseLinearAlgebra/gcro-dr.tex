\subsection{\label{sec:GCRODR}\index{GCRO-DR}\index{GMRES-MDR} GCRO-DR and GMRES-MDR}

`A comparison with the methods seen in the previous chapter indicates that in many cases, GMRES will be faster if the problem is well conditioned, resulting in a moderate number of steps required to converge. If many steps (say, in the hundreds) are required, then BICGSTAB and TFQMR may perform better. If memory is not an issue, GMRES or DQGMRES, with a large number of directions, is often the most reliable choice. The issue then is one of trading ribustness for memory usage. In general, a sound strategy is to focus on finding a good preconditioner rather than the best accelerator'. ~\cite{sparselinearbook1}.

That might because the Krylov space will converge to the domain eigen-vector.

From Fig.~1.~9 of Ref.~\cite{luscher2010}, the \index{low mode}low mode is the most critical problem, so CLGLib first implement low mode \index{deflation}deflation preconditioner\index{preconditioner}.

In the following, we follow Ref.~\cite{deflation}.

\subsubsection{\label{sec:preconditioner}\index{preconditioner}Brief introduction to deflation preconditioner}

In short, the preconditioner means, one solve
\begin{equation}
\begin{split}
&M^{-1}Ax=M^{-1}b,
\end{split}
\end{equation}
or
\begin{equation}
\begin{split}
&\left\{\begin{array}{c} AM^{-1}u=b\\ x=M^{-1}u\end{array}\right.
\end{split}
\end{equation}
instead of $Ax=b$. If $M$ is chosen carefully, it is usually faster.

Now, considering $A\in \mathbb{C}^{n\times n}$ and a matrix $Z\in \mathbb{C}^{n\times k}$ such that $Z=(v_1,v_2,\ldots,v_k)$ and each row is a vector $v_i\in \mathbb{C}^n$ such that $v_i^{\dagger}v_j=\delta _{ij}$. So $Z$ acts like a Unitary matrix $Z^{\dagger}Z=\mathbb{I}^{k\times k}$. Then we can use $Z$ to project $A$ on a subspace, as
\begin{equation}
\begin{split}
&T=Z^{\dagger}AZ,\;\;Z^{\dagger}A=TZ^{\dagger}
\end{split}
\end{equation}
so
\begin{equation}
\begin{split}
&Ax=b \Rightarrow \left(\mathbb{I}+Z\left(T^{-1}-\mathbb{I}\right)Z^{\dagger}\right)Ax= \left(\mathbb{I}+Z\left(T^{-1}-\mathbb{I}\right)Z^{\dagger}\right)b\\
&\Rightarrow \left(A-ZZ^{\dagger}A\right)x+ZT^{-1}Z^{\dagger}Ax=b-ZZ^{\dagger}b + ZT^{-1}Z^{\dagger}b\\
\end{split}
\label{eq.gcrodr.precondition}
\end{equation}

Note that $ZZ^{\dagger}\in \mathbb{C}^{n\times n}$ is not identity matrix (\textbf{also not a unitary matrix, but is an Hermitian matrix}). $T\in \mathbb{C}^{k\times k}$ is a small matrix. And then, one can solve
\begin{equation}
\begin{split}
&ZT^{-1}Z^{\dagger}Ax=ZT^{-1}Z^{\dagger}b ,\;\;\textcolor[rgb]{0,0,1}{Z^{\dagger}A=TZ^{\dagger}}\\
&ZT^{-1}\textcolor[rgb]{0,0,1}{TZ^{\dagger}}x=ZT^{-1}Z^{\dagger}b \\
&x=ZT^{-1}Z^{\dagger}b\\
\end{split}
\label{eq.gcrod.exactsolution}
\end{equation}
exactly, while solving $\left(A-ZZ^{\dagger}A\right)x=b-ZZ^{\dagger}b$ by iteration methods such as GMRES.

This is the so-called \textbf{subspace deflation}\index{deflation}.

\subsubsection{\label{sec:gcrodr}\index{GCRO-DR}Brief intro to GCRO-DR}

Start from Eq.~(\ref{eq,gemres.1}). Assume after the first-step GMRES, we have the orthogonal-normal basis $v_i$ which can be written as a matrix $V_m\in \mathbb{C}^{n\times m},V_{m+1}\in \mathbb{C}^{n\times (m+1)},H\in \mathbb{C}^{(m+1)\times m}$. On the other hand, will be introduced later, we have a set of deflation vectors, or a matrix $P_k\in \mathbb{C}^{\textcolor[rgb]{0,0,1}{m}\times k}$, such that
\begin{equation}
\begin{split}
&AV_mP_k=V_{m+1}HP_k\\
&\tilde {Y}_k\equiv V_mP_k \in \mathbb{C}^{n\times k}\\
\end{split}
\end{equation}
Then $\tilde {Y}_k$ is the deflation matrix.

Consider the matrix $HP_k=QR$, where $QR$ is the QR factorization, with $Q\in \mathbb{C}^{(m+1)\times k}$ and $R\in \mathbb{C}^{k\times k}$. And define
\begin{equation}
\begin{split}
&C_k\equiv V_{m+1}Q\in \mathbb{C}^{n\times k}.
\end{split}
\end{equation}

So, if $R$ which is a small upper triangular matrix such that $R^{-1}$ can be easily calculated, it is
\begin{equation}
\begin{split}
&AV_mP_k=A\tilde{Y}_k=V_{m+1}HP_k=V_{m+1}QR=C_kR\\
&C_k=A\tilde{Y}_kR^{-1}=A U,\;\;\;U\equiv \tilde{Y}_kR^{-1} \in \mathbb{C}^{n\times k}
\end{split}
\end{equation}

Finally, the problem in GMRES Eq.~(\ref{eq,gemres.1}) is changed as
\begin{equation}
\begin{split}
&\tilde{U}_k=U_kD_k=U_k\left(\begin{array}{cccc}
\frac{1}{\|{\bf u}_1\|} & 0 & 0 & 0 \\
0 & \frac{1}{\|{\bf u}_2\|} & 0 & 0 \\
\ldots & \ldots & \ldots & \ldots \\
0 & 0 & 0 & \frac{1}{\|{\bf u}_k\|} \\
\end{array}\right) \in \mathbb{C}^{n\times k}\\
&V^{(1)}_m=(U_k, V_{m-k})\in \mathbb{C}^{n\times m}\\
&V^{(2)}_{m+1}=(C_k, V_{m-k+1})\in \mathbb{C}^{n\times (m+1)}\\
&H'=\left(\begin{array}{cc}
D_k & B_{m-k} \\
0 & H_{m-k}
\end{array}\right) \in \mathbb{C}^{(m+1)\times m}\\
&AV^{(1)}_m=V^{(2)}_{m+1}H'\\
\end{split}
\label{eq.gcrodr.1}
\end{equation}

where $V$ are orthogonal-normal basis obtained in GMRES, and $B_{m-k}=AV_{m-k}$. Note that \textcolor[rgb]{0,0,1}{$B_{m-k}\in \mathbb{C}^{(m-k)\times k}$ but $H_{m-k}\in \mathbb{C}^{(m-k+1)\times (m-k)}$}.

From Eq.~(\ref{eq.gcrodr.1}), we find

\begin{itemize}
  \item The subspace is $k$ dimension subspace of $m$ dimension Krylov space.
  \item With $H$, $V$ and $P$ known, we are able to calculate $QR=HP$, $U=V_mPR^{-1}$, $C=V_{m+1}Q$.
\end{itemize}

\subsubsection{\label{sec:deflationsubspace}The choice of deflation subspace}

In the above, we have assumed $P_k\in \mathbb{C}^{m\times k}$ is already known. Now we concentrate on this part.

Let $A\in \mathbb{C}^{n\times n},V\in \mathbb{C}^{n\times k}$, \textbf{If $V$ is formed as orthogonal normal basis of subspace $S$}, then, if $(\lambda, w\in \mathbb{C}^m)$ is eigen-pair of $V^{\dagger}AV$, $(\lambda, u=V^{\dagger}w\in \mathbb{C}^n)$ is eigen-pair of $A$.

\begin{equation}
\begin{split}
&H_mp_i=\lambda _i p_i,\;\;H_m=V_m^{\dagger}AV_m\\
&A\left(V_mp_i\right)=V_mH_mp_i=\lambda _i \left(V_mp_i\right)\\
\end{split}
\label{eq.gcrodr.subdeflation}
\end{equation}

Therefor, \textcolor[rgb]{0,0,1}{\textbf{$P_k$ is a matrix with $k$ rows, and each row is a eigen-vector of $H_m$ ( $H_m$ denoting the first $m$ row of $H_{m+1}$)}}, then, $VP_k$ is a matrix with $k$ rows such that each row is a eigen-vector of $A$ (approximately since $AV\approx VH \Rightarrow H \approx V^{\dagger}AV$).

The first GMRES cycle will generate $H_m$ (denoting the first $m$ row of $H_{m+1}$), and $H_m\omega=\theta \omega$ is solved. However, starting from the second cycle of GCRO-DR, it is not $AV_m=V_{m+1}H_{m+1}$ but $AV_m^{(1)}=V_{m+1}^{(2)}H'_{m+1}$, such that \textcolor[rgb]{1,0,0}{\textbf{$V_{m+1}^{(2)}$ are orthogonal basis but $V_{m}^{(1)}$ are not orthogonal basis! (Therefor $V^{\dagger}AV$ does not hold!)}}. In this case, it is another eigen-problem which should be solved. This will be listed below without explain.

By Ref.~\cite{deflation}, there are three strategies, \index{Ritz eigen-vector}\index{REV}Ritz eigen-vector~(REV), \index{harmonic Ritz eigen vector}harmonic Ritz eigen vector~(HEV) and \index{singular value decomposition}\index{SVD}singular value decomposition~(SVD). Either it is $REV>HEV>SVD$ or $SVD>HEV>REV$, so we only list REV and SVD here.

\begin{itemize}
  \item REV
\end{itemize}

The $k$ small eigen value of $m$, such that $m$ is

\begin{equation}
\begin{split}
\left\{\begin{array}{l}
H_m\omega = \theta \omega,\\
\left(\begin{array}{cc} \tilde{U}_k^{\dagger}C_k & \tilde{U}_k^{\dagger}V_{m-k+1} \\ 0 & (I_{m-k},0)\end{array}\right) H'_{m+1}\omega = \theta \left(\begin{array}{cc} \tilde{U}_k^{\dagger}\tilde{U}_{k}  & \tilde{U}_k^{\dagger}V_{m-k} \\ V_{m-k}^{\dagger}\tilde{U}_k & I_{m-k}\end{array}\right)\omega,
\end{array}\right.
\end{split}
\end{equation}

\begin{itemize}
  \item SVD
\end{itemize}

The $k$ small eigen value of $m$, such that $m$ is

\begin{equation}
\begin{split}
\left\{\begin{array}{l}
H_m^{\dagger}H_m\omega = \theta \omega,\\
{H'_{m+1}}^{\dagger}H'_{m+1}\omega = \theta \left(\begin{array}{cc} \tilde{U}_k^{\dagger}\tilde{U}_{k}  & 0 \\ 0 & I_{m-k}\end{array}\right)\omega,
\end{array}\right.
\end{split}
\end{equation}

Although $H_m$ is usually a small matrix, we still need to known how to calculate the eigen-value and eigen-vectors.

Note that the second line of REV and SVD, that is a \index{generalized eigen-value problem}\index{GEV}\textbf{generalized eigen-value problem~(GEV)}.

\subsubsection{\label{sec:eigenSolver}Eigen solver}

The eigen solver is implemented following Ref.~\cite{matrixcomputation}.

There are many strategies. The most common algorithm is to transform a matrix to a \index{Hessenberg matrix}\textbf{Hessenberg matrix}.

\begin{itemize}
  \item \index{Householder reflection}Householder reflection
\end{itemize}

Tested that householder reduction is faster than symmetric or unsymmetric Lanczos method when the matrix is large. On the other hand, for a Hermitian matrix, Householder can also produce Hermitian tri-diagonal matrix.

\textbf{Note that this might be not true when the matrix is huge, and Hessenberg reduction is not a full reduction. In our case, we concentrate on matrix with $5<m<50$. Tested when about $7<m<30$ ($30 \times 30 \approx 1024$ is the maximum thread count on test machine), Householder is faster.}

Also, as tested, the quality of QR factorization affects the QR iteration very much. At the same time, compared with QR iteration, the QR factorization is relatively cheap, so we also use Householder to do the QR factorization.

The householder reduction is to insert zeros into a vector, which can be briefly written as
\begin{equation}
\begin{split}
&{\bf v}=\left(\begin{array}{c} x_1+e^{i \arg{x_1}}|{\bf x}|\\x_2\\ \ldots \\ x_n\end{array}\right),\;\;U=\mathbb{I}-\frac{2{\bf v}{\bf v}^{\dagger}}{{\bf v}^{\dagger}{\bf v}},\;\;U\left(\begin{array}{c} x_1\\x_2\\ \ldots \\ x_n\end{array}\right)=\left(\begin{array}{c} \frac{|x_1|}{x_1^*}|{\bf x}|\\0\\ \ldots \\ 0\end{array}\right),
\end{split}
\end{equation}

Note that $U$ is at the same time unitary and Hermitian. Since it is unitary, it can be used as \index{QR factorization}QR factorization, $A=QR$ where $Q$ is unitary and $R$ is upper triangular, and to transform a matrix to Henssenberg matrix $A=U^{\dagger}HU$, where $U$ is unitary and $H$ is upper Henssenberg matrix.

The algorithm is not listed, the procedure can be written as
\begin{equation}
\begin{split}
&A_0=U_0^{\dagger}U_0A_0=U_0^{\dagger}A_1,\;U_0A_0= \left(\mathbb{I}-\frac{2{\bf v}{\bf v}^{\dagger}}{{\bf v}^{\dagger}{\bf v}}\right)A_0=A_1=\left(\begin{array}{cccc}
+ & + & + & + \\ 0 & + & + & + \\ 0 & + & + & + \\ 0 & + & + & +\end{array}\right)\\
&A_0=U_0^{\dagger}U_1^{\dagger}U_1A_1,\; U_1A_1=\left(\begin{array}{cc}\mathbb{I}_1 & 0 \\ 0 & \mathbb{I}-\frac{2{\bf v}{\bf v}^{\dagger}}{{\bf v}^{\dagger}{\bf v}} \end{array}\right)A_1=A_2=\left(\begin{array}{cccc}
+ & + & + & + \\ 0 & + & + & + \\ 0 & 0 & + & + \\ 0 & 0 & + & +\end{array}\right)\\
&A_0=U_0^{\dagger}U_1^{\dagger}U_2^{\dagger}R,\; R=U_2A_2=\left(\begin{array}{cc}\mathbb{I}_2 & 0 \\ 0 & \mathbb{I}-\frac{2{\bf v}{\bf v}^{\dagger}}{{\bf v}^{\dagger}{\bf v}} \end{array}\right)A_2=R=\left(\begin{array}{cccc}
+ & + & + & + \\ 0 & + & + & + \\ 0 & 0 & + & + \\ 0 & 0 & 0 & +\end{array}\right)\\
\end{split}
\end{equation}

Similarly, note that if only insert zeroes from the second row
\begin{equation}
\begin{split}
&UA=\left(\begin{array}{cc}\mathbb{I}_1 & 0 \\ 0 & \mathbb{I}-\frac{2{\bf v}{\bf v}^{\dagger}}{{\bf v}^{\dagger}{\bf v}} \end{array}\right)A=\left(\begin{array}{cccc}
+ & + & + & + \\ + & + & + & + \\ 0 & + & + & + \\ 0 & + & + & +\end{array}\right)\\
\end{split}
\end{equation}
then
\begin{equation}
\begin{split}
&UAU^{\dagger}=UA\left(\begin{array}{cc}\mathbb{I}_1 & 0 \\ 0 & @ \end{array}\right)=\left(\begin{array}{cccc}
+ & + & + & + \\ + & + & + & + \\ 0 & + & + & + \\ 0 & + & + & +\end{array}\right)\left(\begin{array}{cc}\mathbb{I}_1 & 0 \\ 0 & @ \end{array}\right)=\left(\begin{array}{cccc}
+ & +@ & +@ & +@ \\ + & +@ & +@ & +@ \\ 0 & +@ & +@ & +@ \\ 0 & +@ & +@ & +@\end{array}\right)
\end{split}
\end{equation}
So that it is kept Hensenberg.

\begin{itemize}
  \item \index{Shifted QR iteration}Shifted QR iteration
\end{itemize}

Let $A=U_0^{\dagger}H_0U_0$ where $H$ is a Henssenberg matrix, then, let $H_0=U_1R$, it can be shown that $H_1=RU_1=U_1^{\dagger}(U_1R)U_1$ is still a Henssenberg.

Also, $H=U_1H_1U_1^{\dagger}$, so $A=(U_0^{\dagger}U_1)H_1(U_1^{\dagger}U_0)$.

So, $H_1$ has same eigen-value as $A$.

Apart from that, $H_i$ can approach a upper triangular matrix. It is noted that, if the QR factorization is performed to a shifted matrix $H-\sigma I$, where $\sigma$ is an approximate eigen-value of $H$, it will converge much fast.

In CLGLib, we use Wilkinson shift, which is the eigen-value of the right-bottom $2\times 2$ irreducible matrix, and is the one closer to the right-bottom corner element. It can be written as
\begin{algorithm}[H]
\begin{algorithmic}
\For{$H$ is not a triangular}
    \State{$\sigma$ be the eigen-value of the $2\times 2$ matrix of right-bottom matrix which is closer to the right bottom element.}
    \State{$QR=H-\sigma I$}
    \State{$H'=RQ+\sigma I$}
    \If { $H_{n-1,n}\approx 0$}
        \State {Reduce to a $n-1$ Henssenberg matrix problem.}
    \EndIf
\EndFor

\end{algorithmic}
\caption{Shifted QR Iteration}
\end{algorithm}

Once the upper triangular matrix is obtained, the eigen-values are just the diagonal elements.

\begin{itemize}
  \item \index{Inverse power iteration}Inverse power iteration
\end{itemize}

Once the eigen-values are obtained, one can calculate the approximate eigen-vector correspond the the eigen-value using inverse power iteration. The inverse power iteration performs well with the original matrix $A$.
\begin{algorithm}[H]
\begin{algorithmic}
\State {${\bf v}$ is a normalized vector.}
\For{$\|(A-\sigma I){\bf v}\|>\epsilon$}
    \State{$QR=(A-\sigma I)$}
    \State{${\bf v}=R^{-1}Q^{\dagger}{\bf v}$}
    \State{${\bf v}={\bf v}/\|{\bf v}\|$}
\EndFor

\end{algorithmic}
\caption{Inverse power Iteration}
\end{algorithm}

The $R$ is upper triangular, so $R^{-1}$ is just a modification of Algorithm.~\ref{alg.GEMRES.SolveY}.
\begin{algorithm}[H]
\begin{algorithmic}
\For{$i=k-1$ to $0$}
    \For{$j=i+1$ to $k-1$}
        \State ${\bf y}[i]-=r[i,j] {\bf y}[j]$
    \EndFor
    \State ${\bf y}[i] = {\bf y}[i] / r[i,i]$
\EndFor

\Return ${\bf u}[k]={\bf y}[k]$.
\end{algorithmic}
\caption{\label{alg.backsub}\index{backward substitution}Backward substitution}
\end{algorithm}

\begin{itemize}
  \item Eigen vector of upper triangular matrix
\end{itemize}

The inverse power iteration is incompatible with upper triangular matrix, because $R-\lambda I$ is singular, for the inverse power iteration, $R-\lambda I$ is only nearly singular, however, for a upper triangular, it is almost exactly a singular. Although one can shift the eigen value a little bit, but one can also obtain eigen vector exactly. by the procedure below.

Suppose
\begin{equation}
\begin{split}
\left(\begin{array}{ccccc}
r_{1,1}-\lambda_k & \ldots & r_{1,k-1} \\
                0 & \ldots & \ldots    \\
                0 &      0 & r_{k-1,k-1}-\lambda _k
\end{array}\right)
\left(\begin{array}{c} x_1 \\ \ldots \\ x_{k-1} \end{array}\right)=\left(\begin{array}{c} y_1 \\ \ldots \\ y_{k-1} \end{array}\right)
\end{split}
\end{equation}
$(R-\lambda _k \mathbb{I}){\bf x} = 0$ can be written as
\begin{equation}
\begin{split}
&\left(\begin{array}{ccccc}
r_{1,1}-\lambda_k & \ldots & r_{1,k-1}   & r_{1,k}   & \ldots \\
      0 & \ldots & \ldots      & \ldots    & \ldots \\
      0 &      0 & r_{k-1,k-1}-\lambda _k & r_{k-1,k} & \ldots \\
      0 &      0 & 0           & 0         & \ldots \\
      0 &      0 & 0           & 0         & \ldots \\
\end{array}\right)
\left(\begin{array}{c} x_1 \\ \ldots \\ x_{k-1} \\ 1 \\ 0 \\ \ldots \end{array}\right)=
\left(\begin{array}{c} y_1+r_{1,k} \\ \ldots \\ y_{k-1}+r_{k-1,k} \\ 0 \\ \ldots \end{array}\right)=0
\end{split}
\end{equation}
leads to the equation
\begin{equation}
\begin{split}
\left(\begin{array}{ccccc}
r_{1,1}-\lambda_k & \ldots & r_{1,k-1} \\
                0 & \ldots & \ldots    \\
                0 &      0 & r_{k-1,k-1}-\lambda _k
\end{array}\right)
\left(\begin{array}{c} x_1 \\ \ldots \\ x_{k-1} \end{array}\right)=\left(\begin{array}{c} -r_{1,k} \\ \ldots \\ -r_{k-1,k} \end{array}\right)
\end{split}
\end{equation}
which can be solved using backward shift, i.e. Algorithm.~\ref{alg.backsub}.

\begin{itemize}
  \item \index{generalized eigen-value problem}Generalized eigen-value problem
\end{itemize}

The generalized eigen-value problem can be transformed to a eigen-value problem
\begin{equation}
\begin{split}
&A{\bf v}=\lambda B{\bf v} \Rightarrow B=QR \Rightarrow R^{-1}Q^{\dagger}A {\bf v}=\lambda {\bf v}
\end{split}
\end{equation}

\subsubsection{\label{sec:implementationgcrodr}Implementation of GCRO-DR}

Now, we concentrate on the implementation of GCRO-DR. First of all, we need to know how to apply ${\bf x}-AB^{\dagger}{\bf v}$, where $A,B\in \mathbb{C}^{n\times k}$ and ${\bf v}\in \mathbb{C}^n$.
\begin{algorithm}[H]
\begin{algorithmic}
\For{$i=0$ to $k-1$}
    \State{${\bf x}={\bf x}-\left({\bf b}_k^{\dagger}{\bf x}\right){\bf a}_k$}
\EndFor

\Return $\bf x$
\end{algorithmic}
\caption{${\bf x}={\bf x}-AB^{\dagger}{\bf v}$}
\end{algorithm}

The second thing is QR decompose of $\mathbb{C}^{n\times k}$ and $\mathbb{C}^{(m+1)\times k}$ matrix. For the $\mathbb{C}^{n\times k}$ matrix, the usually Arnoldi with modified Gram-Schmidt, i.e. Algorithm.~\ref{alg.ArnoldiModifiedGS} can be used.
\begin{algorithm}[H]
\begin{algorithmic}
\For{$i=0$ to $k-1$}
    \State $y_i=Ay_i$
\EndFor
\State ${\bf v}^{(0)}\:=y_0/\|{\bf y}_0\|$
\For{$i=0$ to $k-1$}
    \State ${\bf w}\:={\bf y}_{i+1}$
    \For{$j=i+1$ to $k-1$}
        \State $c={{\bf v}^{(j)}}^*\cdot {\bf w}$
        \State ${\bf w}-=c{\bf v}^{(j)}$
        \State $r[j,i]=c$
    \EndFor
    \State ${\bf v}^{(i+1)}={\bf w}/r[i+1,i+1]$
\EndFor

\Return {$Q=({\bf v}_0,\ldots, {\bf v}_{k-1})$, $R=r[i,j]$.}
\end{algorithmic}
\caption{modified Gram-Schmidt for QR factorization decompose of $A\tilde{Y}_k$}
\end{algorithm}

Finally we have to calculate $YR^{-1}$. This is a \index{forward substitution}forward substitution.
\begin{equation}
\begin{split}
&U=YR^{-1},\;\; UR=Y,\;\;R^{T}U^{T}=Y^T\;\;U^{T}=\left(R^{T}\right)^{-1}Y^T.\\
\end{split}
\end{equation}

\subsubsection{\label{sec:impgcrodr}\index{GCRO-DR}Implement of GCRO-DR}

The GCRO-DR has four different phases
\begin{enumerate}
  \item Start of the trajectory, use GMRES to solve the first time.
  \item Continue the trajectory, use the last deflation space to form a new deflation space.
  \item First GCRO-DR step.
  \item Restart.
\end{enumerate}

The first step is GMRES, which can be briefly written as
\begin{equation}
\begin{split}
&AV_m=V_{m+1}H_{m+1}\\
\end{split}
\end{equation}
Using $V_{m}^{\dagger}AV_m\approx H_m$ and Eq.~(\ref{eq.gcrodr.subdeflation}), we solve the eigen vectors of $H_m$ denoting as a matrix $P_{m,k}$, $V_mP_{m,k}\in \mathbb{C}^{m\times k}$ are $k$ approximate eigen-vectors of $A$, denoting as $Y_k\in \mathbb{C}^{n\times k}$.

Now we need to project out the subspace $Y_kT^{-1}Y_k^{\dagger} A x= Y_kT^{-1}Y_k^{\dagger}b$, where $T=Y_k^{\dagger} A Y_k$. Let
\begin{equation}
\begin{split}
&QR=AY_k,\;\;C_k=Q\in \mathbb{C}^{n\times k},\;\;U_k=Y_kR^{-1}\in \mathbb{C}^{n\times k}.\\
&AU_k=AY_kR^{-1}=QRR^{-1}=Q=C_k\\
&T^{-1}=\left(Y_k^{\dagger} A Y_k\right)^{-1}=\left(Y_k^{\dagger} C_k R\right)^{-1}=R^{-1}\left(Y_k^{\dagger} C_k\right)^{-1}\\
\end{split}
\end{equation}
Also if $Y_k$ are eigen-vectors
\begin{equation}
\begin{split}
&AY_k\approx Y_k\Lambda,\;\;\Lambda = \left(\begin{array}{cccc}
\lambda _1 & 0 & \ldots & 0 \\
0 & \lambda _2 & \ldots & 0 \\
\ldots & \ldots & \ldots & \ldots \\
0 & \ldots & 0 & \lambda_k \\
\end{array}\right)\\
\end{split}
\end{equation}
Note that $Y_k\Lambda $ is also QR factorization of $AY_k$, so
\begin{equation}
\begin{split}
&Y_k\approx C_k,\;\;\left(Y_k^{\dagger}C_k\right)^{-1}Y_k^{\dagger}\approx C_k^{\dagger}\\
\end{split}
\end{equation}
so Using Eq.~(\ref{eq.gcrod.exactsolution}), so
\begin{equation}
\begin{split}
&x_d=Y_kT^{-1}Y_k^{\dagger}r_0=Y_kR^{-1}\left(Y_k^{\dagger}C_k\right)^{-1}Y_k^{\dagger}r_0\approx U_kC_k^{\dagger}r_0\\
\end{split}
\label{eq.gcrod.exactsolution2}
\end{equation}

If we denote the solution of $A(x+x_0)=b$, let $r_0=b-Ax_0$, we are solving $Ax=r_0$. It has same solution as preconditioned equation, i.e., Eq.~(\ref{eq.gcrodr.precondition})
\begin{equation}
\begin{split}
&\left(\mathbb{I}-Y_kY_k^{\dagger}\right)Ax+Y_kT^{-1}Y_k^{\dagger}Ax=\left(\mathbb{I}-Y_kY_k^{\dagger}\right)r_0+Y_kT^{-1}Y_k^{\dagger}r_0\\
&A_1=\left(\mathbb{I}-Y_kY_k^{\dagger}\right)A,\;\;A_2=Y_kT^{-1}Y_k^{\dagger}A\\
&Z_1=\left(\mathbb{I}-Y_kY_k^{\dagger}\right),\;\;Z_2=Y_kT^{-1}Y_k^{\dagger}\\
\end{split}
\end{equation}
Using $Z_1A_2=Z_2A_1=Z_1Z_2=Z_2Z_1=0$, we can use $Z_{1,2}$ to project out two equations
\begin{equation}
\begin{split}
&x=x_1+x_d\\
&A_1x_1=Z_1r_0,\;\;A_2x_d=Z_2r_0\\
\end{split}
\end{equation}
where the solution of $A_2x_d=Z_2r_0$ is just Eq.~(\ref{eq.gcrod.exactsolution2}). The remaining equation is

\begin{equation}
\begin{split}
&x_1=x-x_d,\\
&x_1=x-U_kC_k^{\dagger}r_0\\
&\left(\mathbb{I}-Y_kY_k^{\dagger}\right)Ax_1=r_1=r_0-Y_kY_k^{\dagger}r_0\\
\end{split}
\end{equation}

\paragraph{First GCRO-DR step}

\begin{equation}
\begin{split}
&G(H_my-y_0)=0\\
&(GH_my-Gy_0)=0\\
&H_m\left((GH_m)^{-1}(Gy_0)\right)=(Gy_0)_{m+1}\\
\end{split}
\end{equation}

\paragraph{Restart GCRO-DR step}

\paragraph{Continue the trajectory}
