\subsection{\label{sec:GCRODR}\index{GCRO-DR}\index{GMRES-MDR} GCRO-DR and GMRES-MDR}

`A comparison with the methods seen in the previous chapter indicates that in many cases, GMRES will be faster if the problem is well conditioned, resulting in a moderate number of steps required to converge. If many steps (say, in the hundreds) are required, then BICGSTAB and TFQMR may perform better. If memory is not an issue, GMRES or DQGMRES, with a large number of directions, is often the most reliable choice. The issue then is one of trading ribustness for memory usage. In general, a sound strategy is to focus on finding a good preconditioner rather than the best accelerator'. ~\cite{sparselinearbook1}.

That might because the Krylov space will converge to the domain eigen-vector.

From Fig.~1.~9 of Ref.~\cite{luscher2010}, the \index{low mode}low mode is the most critical problem, so CLGLib first implement low mode \index{deflation}deflation preconditioner\index{preconditioner}.

In the following, we follow Ref.~\cite{deflation}.

\subsubsection{\label{sec:preconditioner}\index{preconditioner}Brief introduction to deflation preconditioner}

In short, the preconditioner means, one solve
\begin{equation}
\begin{split}
&M^{-1}Ax=M^{-1}b,
\end{split}
\end{equation}
or
\begin{equation}
\begin{split}
&\left\{\begin{array}{c} AM^{-1}u=b\\ x=M^{-1}u\end{array}\right.
\end{split}
\end{equation}
instead of $Ax=b$. If $M$ is chosen carefully, it is usually faster.

Now, considering $A\in \mathbb{C}^{n\times n}$ and a matrix $Z\in \mathbb{C}^{n\times k}$ such that $Z=(v_1,v_2,\ldots,v_k)$ and each row is a vector $v_i\in \mathbb{C}^n$ such that $v_i^{\dagger}v_j=\delta _{ij}$. So $Z$ acts like a Unitary matrix $Z^{\dagger}Z=\mathbb{I}^{k\times k}$. Then we can use $Z$ to project $A$ on a subspace, as
\begin{equation}
\begin{split}
&T=Z^{\dagger}AZ,\;\;Z^{\dagger}A=TZ^{\dagger}
\end{split}
\end{equation}
so
\begin{equation}
\begin{split}
&Ax=b \Rightarrow \left(\mathbb{I}+Z\left(T^{-1}-\mathbb{I}\right)Z^{\dagger}\right)Ax= \left(\mathbb{I}+Z\left(T^{-1}-\mathbb{I}\right)Z^{\dagger}\right)b\\
&\Rightarrow \left(A-ZZ^{\dagger}A\right)x+ZT^{-1}Z^{\dagger}Ax=b-ZZ^{\dagger}b + ZT^{-1}Z^{\dagger}b\\
\end{split}
\label{eq.gcrodr.precondition}
\end{equation}

Note that $ZZ^{\dagger}\in \mathbb{C}^{n\times n}$ is not identity matrix (\textbf{also not a unitary matrix, but is an Hermitian matrix}). $T\in \mathbb{C}^{k\times k}$ is a small matrix. And then, one can solve
\begin{equation}
\begin{split}
&ZT^{-1}Z^{\dagger}Ax=ZT^{-1}Z^{\dagger}b ,\;\;\textcolor[rgb]{0,0,1}{Z^{\dagger}A=TZ^{\dagger}}\\
&ZT^{-1}\textcolor[rgb]{0,0,1}{TZ^{\dagger}}x=ZT^{-1}Z^{\dagger}b \\
&x=ZT^{-1}Z^{\dagger}b\\
\end{split}
\label{eq.gcrod.exactsolution}
\end{equation}
exactly, while solving $\left(A-ZZ^{\dagger}A\right)x=b-ZZ^{\dagger}b$ by iteration methods such as GMRES.

This is the so-called \textbf{subspace deflation}\index{deflation}.

\subsubsection{\label{sec:gcrodr}\index{GCRO-DR}Brief intro to GCRO-DR}

Start from Eq.~(\ref{eq,gemres.1}). Assume after the first-step GMRES, we have the orthogonal-normal basis $v_i$ which can be written as a matrix $V_m\in \mathbb{C}^{n\times m},V_{m+1}\in \mathbb{C}^{n\times (m+1)},H\in \mathbb{C}^{(m+1)\times m}$. On the other hand, will be introduced later, we have a set of deflation vectors, or a matrix $P_k\in \mathbb{C}^{\textcolor[rgb]{0,0,1}{m}\times k}$, such that
\begin{equation}
\begin{split}
&AV_mP_k=V_{m+1}HP_k\\
&\tilde {Y}_k\equiv V_mP_k \in \mathbb{C}^{n\times k}\\
\end{split}
\end{equation}
Then $\tilde {Y}_k$ is the deflation matrix.

Consider the matrix $HP_k=QR$, where $QR$ is the QR factorization, with $Q\in \mathbb{C}^{(m+1)\times k}$ and $R\in \mathbb{C}^{k\times k}$. And define
\begin{equation}
\begin{split}
&C_k\equiv V_{m+1}Q\in \mathbb{C}^{n\times k}.
\end{split}
\end{equation}

So, if $R$ which is a small upper triangular matrix such that $R^{-1}$ can be easily calculated, it is
\begin{equation}
\begin{split}
&AV_mP_k=A\tilde{Y}_k=V_{m+1}HP_k=V_{m+1}QR=C_kR\\
&C_k=A\tilde{Y}_kR^{-1}=A U,\;\;\;U\equiv \tilde{Y}_kR^{-1} \in \mathbb{C}^{n\times k}
\end{split}
\end{equation}

Finally, the problem in GMRES Eq.~(\ref{eq,gemres.1}) is changed as
\begin{equation}
\begin{split}
&\tilde{U}_k=U_kD_k=U_k\left(\begin{array}{cccc}
\frac{1}{\|{\bf u}_1\|} & 0 & 0 & 0 \\
0 & \frac{1}{\|{\bf u}_2\|} & 0 & 0 \\
\ldots & \ldots & \ldots & \ldots \\
0 & 0 & 0 & \frac{1}{\|{\bf u}_k\|} \\
\end{array}\right) \in \mathbb{C}^{n\times k}\\
&V^{(1)}_m=(U_k, V_{m-k})\in \mathbb{C}^{n\times m}\\
&V^{(2)}_{m+1}=(C_k, V_{m-k+1})\in \mathbb{C}^{n\times (m+1)}\\
&H'=\left(\begin{array}{cc}
D_k & B_{m-k} \\
0 & H_{m-k}
\end{array}\right) \in \mathbb{C}^{(m+1)\times m}\\
&AV^{(1)}_m=V^{(2)}_{m+1}H'\\
\end{split}
\label{eq.gcrodr.1}
\end{equation}

where $V$ are orthogonal-normal basis obtained in GMRES, and $B_{m-k}=AV_{m-k}$. Note that \textcolor[rgb]{0,0,1}{$B_{m-k}\in \mathbb{C}^{(m-k)\times k}$ but $H_{m-k}\in \mathbb{C}^{(m-k+1)\times (m-k)}$}.

From Eq.~(\ref{eq.gcrodr.1}), we find

\begin{itemize}
  \item The subspace is $k$ dimension subspace of $m$ dimension Krylov space.
  \item With $H$, $V$ and $P$ known, we are able to calculate $QR=HP$, $U=V_mPR^{-1}$, $C=V_{m+1}Q$.
\end{itemize}

\subsubsection{\label{sec:deflationsubspace}The choice of deflation subspace}

In the above, we have assumed $P_k\in \mathbb{C}^{m\times k}$ is already known. Now we concentrate on this part.

Let $A\in \mathbb{C}^{n\times n},V\in \mathbb{C}^{n\times k}$, \textbf{If $V$ is formed as orthogonal normal basis of subspace $S$}, then, if $(\lambda, w\in \mathbb{C}^m)$ is eigen-pair of $V^{\dagger}AV$, $(\lambda, u=V^{\dagger}w\in \mathbb{C}^n)$ is eigen-pair of $A$.

\begin{equation}
\begin{split}
&H_mp_i=\lambda _i p_i,\;\;H_m=V_m^{\dagger}AV_m\\
&A\left(V_mp_i\right)=V_mH_mp_i=\lambda _i \left(V_mp_i\right)\\
\end{split}
\label{eq.gcrodr.subdeflation}
\end{equation}

Therefor, \textcolor[rgb]{0,0,1}{\textbf{$P_k$ is a matrix with $k$ rows, and each row is a eigen-vector of $H_m$ ( $H_m$ denoting the first $m$ row of $H_{m+1}$)}}, then, $VP_k$ is a matrix with $k$ rows such that each row is a eigen-vector of $A$ (approximately since $AV\approx VH \Rightarrow H \approx V^{\dagger}AV$).

The first GMRES cycle will generate $H_m$ (denoting the first $m$ row of $H_{m+1}$), and $H_m\omega=\theta \omega$ is solved. However, starting from the second cycle of GCRO-DR, it is not $AV_m=V_{m+1}H_{m+1}$ but $AV_m^{(1)}=V_{m+1}^{(2)}H'_{m+1}$, such that \textcolor[rgb]{1,0,0}{\textbf{$V_{m+1}^{(2)}$ are orthogonal basis but $V_{m}^{(1)}$ are not orthogonal basis! (Therefor $V^{\dagger}AV$ does not hold!)}}. In this case, it is another eigen-problem which should be solved. This will be listed below without explain.

By Ref.~\cite{deflation}, there are three strategies, \index{Ritz eigen-vector}\index{REV}Ritz eigen-vector~(REV), \index{harmonic Ritz eigen vector}harmonic Ritz eigen vector~(HEV) and \index{singular value decomposition}\index{SVD}singular value decomposition~(SVD). Either it is $REV>HEV>SVD$ or $SVD>HEV>REV$, so we only list REV and SVD here.

\textcolor[rgb]{0,0,1}{Note that $\tilde{U}_k$ is the normalized $U_k$, $H_{m+1}$ means the $H_{m+1}$ of GMRES procedure, and ${H'_{m+1}}$ means ${H'_{m+1}}$ obtained in GCRO-DR procedure, $H_{m}$ and ${H'_{m}}$ means the upper $m$ rows of $H_{m+1}$ and ${H'_{m+1}}$.}

\begin{itemize}
  \item REV
\end{itemize}

The $k$ small eigen value of $m$, such that $m$ is

\begin{equation}
\begin{split}
\left\{\begin{array}{l}
H_m\omega = \theta \omega,\\
\left(\begin{array}{cc} \tilde{U}_k^{\dagger}C_k & \tilde{U}_k^{\dagger}V_{m-k+1} \\ 0 & (I_{m-k},0)\end{array}\right) H'_{m+1}\omega = \theta \left(\begin{array}{cc} \tilde{U}_k^{\dagger}\tilde{U}_{k}  & \tilde{U}_k^{\dagger}V_{m-k} \\ V_{m-k}^{\dagger}\tilde{U}_k & I_{m-k}\end{array}\right)\omega,
\end{array}\right.
\end{split}
\end{equation}

\begin{itemize}
  \item HEV
\end{itemize}

The $k$ \textbf{larger} eigen value of $m$, such that $m$ is

\begin{equation}
\begin{split}
\left\{\begin{array}{l}
{H_{m}}^{\dagger}\omega = \theta {H_{m+1}}^{\dagger}{H_{m+1}}\omega,\\
{H'_{m+1}}^{\dagger}\left(\begin{array}{cc} C_k^{\dagger}\tilde{U}_k  & 0 \\ V_{m-k+1}^{\dagger}\tilde{U}_k  & \left(\begin{array}{c}I_{m-k} \\ 0\end{array}\right)\end{array}\right)\omega = \theta {H'_{m+1}}^{\dagger}{H'_{m+1}}\omega,
\end{array}\right.
\end{split}
\end{equation}

\begin{itemize}
  \item SVD
\end{itemize}

The $k$ small eigen value of $m$, such that $m$ is

\begin{equation}
\begin{split}
\left\{\begin{array}{l}
H_m^{\dagger}H_m\omega = \theta \omega,\\
{H'_{m+1}}^{\dagger}H'_{m+1}\omega = \theta \left(\begin{array}{cc} \tilde{U}_k^{\dagger}\tilde{U}_{k}  & 0 \\ 0 & I_{m-k}\end{array}\right)\omega,
\end{array}\right.
\end{split}
\end{equation}

Although $H_m$ is usually a small matrix, we still need to known how to calculate the eigen-value and eigen-vectors.

Note that the second line of REV and SVD, and both line of HEV, that is a \index{generalized eigen-value problem}\index{GEV}\textbf{generalized eigen-value problem~(GEV)}.

\subsubsection{\label{sec:eigenSolver}Eigen solver}

The eigen solver is implemented following Ref.~\cite{matrixcomputation}.

There are many strategies. The most common algorithm is to transform a matrix to a \index{Hessenberg matrix}\textbf{Hessenberg matrix}.

\begin{itemize}
  \item \index{Householder reflection}Householder reflection
\end{itemize}

Tested that householder reduction is faster than symmetric or unsymmetric Lanczos method when the matrix is large. On the other hand, for a Hermitian matrix, Householder can also produce Hermitian tri-diagonal matrix.

\textbf{Note that this might be not true when the matrix is huge, and Hessenberg reduction is not a full reduction. In our case, we concentrate on matrix with $5<m<50$. Tested when about $7<m<30$ ($30 \times 30 \approx 1024$ is the maximum thread count on test machine), Householder is faster.}

Also, as tested, the quality of QR factorization affects the QR iteration very much. At the same time, compared with QR iteration, the QR factorization is relatively cheap, so we also use Householder to do the QR factorization.

The householder reduction is to insert zeros into a vector, which can be briefly written as
\begin{equation}
\begin{split}
&{\bf v}=\left(\begin{array}{c} x_1+e^{i \arg{x_1}}|{\bf x}|\\x_2\\ \ldots \\ x_n\end{array}\right),\;\;U=\mathbb{I}-\frac{2{\bf v}{\bf v}^{\dagger}}{{\bf v}^{\dagger}{\bf v}},\;\;U\left(\begin{array}{c} x_1\\x_2\\ \ldots \\ x_n\end{array}\right)=\left(\begin{array}{c} \frac{|x_1|}{x_1^*}|{\bf x}|\\0\\ \ldots \\ 0\end{array}\right),
\end{split}
\end{equation}

Note that $U$ is at the same time unitary and Hermitian. Since it is unitary, it can be used as \index{QR factorization}QR factorization, $A=QR$ where $Q$ is unitary and $R$ is upper triangular, and to transform a matrix to Henssenberg matrix $A=U^{\dagger}HU$, where $U$ is unitary and $H$ is upper Henssenberg matrix.

The algorithm is not listed, the procedure can be written as
\begin{equation}
\begin{split}
&A_0=U_0^{\dagger}U_0A_0=U_0^{\dagger}A_1,\;U_0A_0= \left(\mathbb{I}-\frac{2{\bf v}{\bf v}^{\dagger}}{{\bf v}^{\dagger}{\bf v}}\right)A_0=A_1=\left(\begin{array}{cccc}
+ & + & + & + \\ 0 & + & + & + \\ 0 & + & + & + \\ 0 & + & + & +\end{array}\right)\\
&A_0=U_0^{\dagger}U_1^{\dagger}U_1A_1,\; U_1A_1=\left(\begin{array}{cc}\mathbb{I}_1 & 0 \\ 0 & \mathbb{I}-\frac{2{\bf v}{\bf v}^{\dagger}}{{\bf v}^{\dagger}{\bf v}} \end{array}\right)A_1=A_2=\left(\begin{array}{cccc}
+ & + & + & + \\ 0 & + & + & + \\ 0 & 0 & + & + \\ 0 & 0 & + & +\end{array}\right)\\
&A_0=U_0^{\dagger}U_1^{\dagger}U_2^{\dagger}R,\; R=U_2A_2=\left(\begin{array}{cc}\mathbb{I}_2 & 0 \\ 0 & \mathbb{I}-\frac{2{\bf v}{\bf v}^{\dagger}}{{\bf v}^{\dagger}{\bf v}} \end{array}\right)A_2=R=\left(\begin{array}{cccc}
+ & + & + & + \\ 0 & + & + & + \\ 0 & 0 & + & + \\ 0 & 0 & 0 & +\end{array}\right)\\
\end{split}
\end{equation}

Similarly, note that if only insert zeroes from the second row
\begin{equation}
\begin{split}
&UA=\left(\begin{array}{cc}\mathbb{I}_1 & 0 \\ 0 & \mathbb{I}-\frac{2{\bf v}{\bf v}^{\dagger}}{{\bf v}^{\dagger}{\bf v}} \end{array}\right)A=\left(\begin{array}{cccc}
+ & + & + & + \\ + & + & + & + \\ 0 & + & + & + \\ 0 & + & + & +\end{array}\right)\\
\end{split}
\end{equation}
then
\begin{equation}
\begin{split}
&UAU^{\dagger}=UA\left(\begin{array}{cc}\mathbb{I}_1 & 0 \\ 0 & @ \end{array}\right)=\left(\begin{array}{cccc}
+ & + & + & + \\ + & + & + & + \\ 0 & + & + & + \\ 0 & + & + & +\end{array}\right)\left(\begin{array}{cc}\mathbb{I}_1 & 0 \\ 0 & @ \end{array}\right)=\left(\begin{array}{cccc}
+ & +@ & +@ & +@ \\ + & +@ & +@ & +@ \\ 0 & +@ & +@ & +@ \\ 0 & +@ & +@ & +@\end{array}\right)
\end{split}
\end{equation}
So that it is kept Hensenberg.

\begin{itemize}
  \item \index{Shifted QR iteration}Shifted QR iteration
\end{itemize}

Let $A=U_0^{\dagger}H_0U_0$ where $H$ is a Henssenberg matrix, then, let $H_0=U_1R$, it can be shown that $H_1=RU_1=U_1^{\dagger}(U_1R)U_1$ is still a Henssenberg.

Also, $H=U_1H_1U_1^{\dagger}$, so $A=(U_0^{\dagger}U_1)H_1(U_1^{\dagger}U_0)$.

So, $H_1$ has same eigen-value as $A$.

Apart from that, $H_i$ can approach a upper triangular matrix. It is noted that, if the QR factorization is performed to a shifted matrix $H-\sigma I$, where $\sigma$ is an approximate eigen-value of $H$, it will converge much fast.

In CLGLib, we use Wilkinson shift, which is the eigen-value of the right-bottom $2\times 2$ irreducible matrix, and is the one closer to the right-bottom corner element. It can be written as
\begin{algorithm}[H]
\begin{algorithmic}
\For{$H$ is not a triangular}
    \State{$\sigma$ be the eigen-value of the $2\times 2$ matrix of right-bottom matrix which is closer to the right bottom element.}
    \State{$QR=H-\sigma I$}
    \State{$H'=RQ+\sigma I$}
    \If { $H_{n-1,n}\approx 0$}
        \State {Reduce to a $n-1$ Henssenberg matrix problem.}
    \EndIf
\EndFor

\end{algorithmic}
\caption{Shifted QR Iteration}
\end{algorithm}

Once the upper triangular matrix is obtained, the eigen-values are just the diagonal elements.

\begin{itemize}
  \item \index{Implicit shifted QR iteration}Implicit shifted QR iteration
\end{itemize}

The \index{Implicit shifted QR iteration}\textbf{Implicit shifted QR iteration} sometimes also called \index{Double shifted QR iteration}\textbf{Double shifted QR iteration} or \index{Francis QR iteration}\textbf{Double shifted QR iteration}.

The details are not listed here, it uses Householder to chase the zero to the bottom and right, it is a little bit better convergent, and is said to be more stable. It can be written as
\begin{algorithm}[H]
\begin{algorithmic}
\For{$T$ a Hessenberg matrix with $n\geq 3$. (In the case of $n=2$, the eigen-value can be directly obtained.)}
    \State{$H$ a irreducible Hessenberg matrix with $n\geq 3$. $T=\left(\begin{array}{ccc} + & + & + \\ 0 & H & + \\ 0 & 0 & + \end{array}\right)$}
    \Comment {In the case of $n=2$, the eigen-value can be directly obtained.}
    \State{$H_{2\times 2}$ be the $2\times 2$ matrix of right-bottom matrix. $s={\rm tr}(H_{2\times 2})$ and $t=\det (H_{2\times 2})$}
    \State{$x=H_{1,1}(H_{1,1}-s)+H_{1,2}H_{2,1}+t$}
    \State{$y=H_{2,1}(H_{1,1}+H_{2,2}-s)$}
    \State{$z=H_{2,1}H_{3,2}$}        
    \State{$H'=RQ+\sigma I$}
    \For { $k=0$ to $n-3$}
        \State {$h$ be Householder matrix to zero ${\bf v}=(x,y,z)^T\to (|{\bf v}|,0,0)^T$.}
        \State {$q=\max (1,k)$, $H(k+1:k+3,q:n)=h H(k+1:k+3,q:n)$.}
        \State {$r=\min (k+4,n)$, $H(1:4,k+1:k+3)=H(1:4,k+1:k+3)h^{\dagger}$.}        
        \Comment {Note that $h^{\dagger}=h$}
        \State {$x=H(k+2,k+1),y=H(k+3,k+1)$}
        \If { $k<n-3$ }
            \State {$z=H(k+4,k+1)$}        
        \EndIf
    \EndFor
    \State {$h$ be Householder matrix to zero ${\bf v}=(x,y)^T\to (|{\bf v}|,0)^T$.}
    \State {$H(n-1:n,n-2:n)=h H(n-1:n,n-2:n)$, $H(1:n,n-1:n)=H(1:n,n-1:n)h^{\dagger}$.}
\EndFor

\end{algorithmic}
\caption{Implicit shifted QR iteration}
\end{algorithm}

\begin{itemize}
  \item \index{Inverse power iteration}Inverse power iteration
\end{itemize}

Once the eigen-values are obtained, one can calculate the approximate eigen-vector correspond the the eigen-value using inverse power iteration. The inverse power iteration performs well with the original matrix $A$.
\begin{algorithm}[H]
\begin{algorithmic}
\State {${\bf v}$ is a normalized vector.}
\For{$\|(A-\sigma I){\bf v}\|>\epsilon$}
    \State{$QR=(A-\sigma I)$}
    \State{${\bf v}=R^{-1}Q^{\dagger}{\bf v}$}
    \State{${\bf v}={\bf v}/\|{\bf v}\|$}
\EndFor

\end{algorithmic}
\caption{Inverse power Iteration}
\end{algorithm}

The $R$ is upper triangular, so $R^{-1}$ is just a modification of Algorithm.~\ref{alg.GEMRES.SolveY}.
\begin{algorithm}[H]
\begin{algorithmic}
\For{$i=k-1$ to $0$}
    \For{$j=i+1$ to $k-1$}
        \State ${\bf y}[i]-=r[i,j] {\bf y}[j]$
    \EndFor
    \State ${\bf y}[i] = {\bf y}[i] / r[i,i]$
\EndFor

\Return ${\bf u}[k]={\bf y}[k]$.
\end{algorithmic}
\caption{\label{alg.backsub}\index{backward substitution}Backward substitution}
\end{algorithm}

\begin{itemize}
  \item Eigen vector of upper triangular matrix
\end{itemize}

The inverse power iteration is incompatible with upper triangular matrix, because $R-\lambda I$ is singular, for the inverse power iteration, $R-\lambda I$ is only nearly singular, however, for a upper triangular, it is almost exactly a singular. Although one can shift the eigen value a little bit, but one can also obtain eigen vector exactly. by the procedure below.

Suppose
\begin{equation}
\begin{split}
\left(\begin{array}{ccccc}
r_{1,1}-\lambda_k & \ldots & r_{1,k-1} \\
                0 & \ldots & \ldots    \\
                0 &      0 & r_{k-1,k-1}-\lambda _k
\end{array}\right)
\left(\begin{array}{c} x_1 \\ \ldots \\ x_{k-1} \end{array}\right)=\left(\begin{array}{c} y_1 \\ \ldots \\ y_{k-1} \end{array}\right)
\end{split}
\end{equation}
$(R-\lambda _k \mathbb{I}){\bf x} = 0$ can be written as
\begin{equation}
\begin{split}
&\left(\begin{array}{ccccc}
r_{1,1}-\lambda_k & \ldots & r_{1,k-1}   & r_{1,k}   & \ldots \\
      0 & \ldots & \ldots      & \ldots    & \ldots \\
      0 &      0 & r_{k-1,k-1}-\lambda _k & r_{k-1,k} & \ldots \\
      0 &      0 & 0           & 0         & \ldots \\
      0 &      0 & 0           & 0         & \ldots \\
\end{array}\right)
\left(\begin{array}{c} x_1 \\ \ldots \\ x_{k-1} \\ 1 \\ 0 \\ \ldots \end{array}\right)=
\left(\begin{array}{c} y_1+r_{1,k} \\ \ldots \\ y_{k-1}+r_{k-1,k} \\ 0 \\ \ldots \end{array}\right)=0
\end{split}
\end{equation}
leads to the equation
\begin{equation}
\begin{split}
\left(\begin{array}{ccccc}
r_{1,1}-\lambda_k & \ldots & r_{1,k-1} \\
                0 & \ldots & \ldots    \\
                0 &      0 & r_{k-1,k-1}-\lambda _k
\end{array}\right)
\left(\begin{array}{c} x_1 \\ \ldots \\ x_{k-1} \end{array}\right)=\left(\begin{array}{c} -r_{1,k} \\ \ldots \\ -r_{k-1,k} \end{array}\right)
\end{split}
\end{equation}
which can be solved using backward shift, i.e. Algorithm.~\ref{alg.backsub}.

\begin{itemize}
  \item \index{generalized eigen-value problem}Generalized eigen-value problem
\end{itemize}

The generalized eigen-value problem can be transformed to a eigen-value problem
\begin{equation}
\begin{split}
&A{\bf v}=\lambda B{\bf v} \Rightarrow B=QR \Rightarrow R^{-1}Q^{\dagger}A {\bf v}=\lambda {\bf v}
\end{split}
\end{equation}

\subsubsection{\label{sec:implementationgcrodr}Implementation of GCRO-DR}

Now, we concentrate on the implementation of GCRO-DR. First of all, we need to know how to apply ${\bf x}-AB^{\dagger}{\bf v}$, where $A,B\in \mathbb{C}^{n\times k}$ and ${\bf v}\in \mathbb{C}^n$.
\begin{algorithm}[H]
\begin{algorithmic}
\For{$i=0$ to $k-1$}
    \State{${\bf x}={\bf x}-\left({\bf b}_k^{\dagger}{\bf x}\right){\bf a}_k$}
\EndFor

\Return $\bf x$
\end{algorithmic}
\caption{${\bf x}={\bf x}-AB^{\dagger}{\bf v}$}
\end{algorithm}

The second thing is QR decompose of $\mathbb{C}^{n\times k}$ and $\mathbb{C}^{(m+1)\times k}$ matrix. For the $\mathbb{C}^{n\times k}$ matrix, the usually Arnoldi with modified Gram-Schmidt, i.e. Algorithm.~\ref{alg.ArnoldiModifiedGS} can be used.
\begin{algorithm}[H]
\begin{algorithmic}
\For{$i=0$ to $k-1$}
    \State $y_i=Ay_i$
\EndFor
\State ${\bf v}^{(0)}\:=y_0/\|{\bf y}_0\|$
\For{$i=0$ to $k-1$}
    \State ${\bf w}\:={\bf y}_{i+1}$
    \For{$j=i+1$ to $k-1$}
        \State $c={{\bf v}^{(j)}}^*\cdot {\bf w}$
        \State ${\bf w}-=c{\bf v}^{(j)}$
        \State $r[j,i]=c$
    \EndFor
    \State ${\bf v}^{(i+1)}={\bf w}/r[i+1,i+1]$
\EndFor

\Return {$Q=({\bf v}_0,\ldots, {\bf v}_{k-1})$, $R=r[i,j]$.}
\end{algorithmic}
\caption{modified Gram-Schmidt for QR factorization decompose of $A\tilde{Y}_k$}
\end{algorithm}

Finally we have to calculate $YR^{-1}$. This is a \index{forward substitution}forward substitution.
\begin{equation}
\begin{split}
&U=YR^{-1},\;\; UR=Y,\;\;R^{T}U^{T}=Y^T\;\;U^{T}=\left(R^{T}\right)^{-1}Y^T.\\
\end{split}
\end{equation}

\subsubsection{\label{sec:impgcrodr}\index{GCRO-DR}Implement of GCRO-DR}

We present pseudo-code of GCRO-DR can be found in Ref.~\cite{deflation}. The only difference is that we always make sure $C_k$ and $V_{m-k+1}$ are orthogonal to each other. It can be written as
\begin{algorithm}[H]
\begin{algorithmic}
\If {$U_k$ is defined from solving a previous linear system}
    \State { Let $[Q,R]=AU_k$ be QR decomposition or $AU_k$. }
    \State { $C_k=Q$. }
    \State { $U_k=U_kR^{-1}$. }
    \State { ${\bf r}^{(0)}=A{\bf x}^{(0)}-{\bf b}$}
\Else
    \State {Perform GMRES to get $W_{m+1}=\left(C_k,V_{m-k+1}\right)$}
    \State {Update ${\bf x}^{(0)},{\bf r}^{(0)}$ as ${\bf x}^{(0)}={\bf x}^{(0)}+V_my,{\bf r}^{(0)}=V_{m+1}(\beta {\bf e}_1-H_{m+1} y)$, which is in fact part of GMRES.}
    \State {Compute eigen-vector problem and obtain $P_k\in \mathbb{C}^{m\times k}$. }    
    \State { $U_k=V_mP_k$ }    
    \State { Let $[Q,R]=H_{m+1}P_k$ be QR decomposition. }
    \State { $C_k=V_{m+1}Q$ }    
    \State { $U_k=U_kR^{-1}$ }            
\EndIf
\For { $\hat{i}=1$ to $r$}
    \Comment {restart $r$ times. }
    \State { ${\bf x}^{(i-1)}={\bf x}^{(i-1)}+U_kC_k^{\dagger}{\bf r}^{(i-1)}$}
    \State { ${\bf r}^{(i-1)}={\bf r}^{(i-1)}-C_kC_k^{\dagger}{\bf r}^{(i-1)}$}
    \State { Reset $H_{m+1}=0$. $W_{m+1}(k)=V_{m-k+1}(0)={\bf r}^{(i)}/\|{\bf r}^{(i)}\|$.}
    \State { $H_{m+1}(k,k)=1/\|U_k\|$, normalize $U_k$.}
    \State { Perform Arnoldi procedure on matrix $(1-CC^{\dagger})A$, to obtain $V_{m-k+1}$, and set $H_{k:m+1,k:m}$. And $H_{0:k,m}=C_k^{\dagger}AV_{m-k}$. $\hat{V}=(U_k,V_{m-k})$ and $W_{m+1}=(C_k, V_{m-k+1})$. }
    \State {Solve $\arg \;\min \; \left\|\|r\|{\bf e}_k - H_{m+1}y\right\|$.}
    \State {${\bf x}^{(i)}={\bf x}^{(i-1)}+\hat{V}_my,\;\;{\bf r}^{(i)}={\bf r}^{(i-1)}-W_{m+1}H_{m+1}y)$.}
    \Comment {Check the error here. If reach the criterion, return.}
    \State {Compute eigen-vector problem and obtain $P_k\in \mathbb{C}^{m\times k}$. }
    \State { $U_k=V_mP_k$ }
    \State { Let $[Q,R]=H_{m+1}P_k$ be QR decomposition. }
    \State { $C_k=V_{m+1}Q$ }
    \State { $U_k=U_kR^{-1}$ }      
\EndFor
\end{algorithmic}
\caption{GCRO-DR}
\end{algorithm}

\subsubsection{\label{sec:impgmresmdr}\index{GMRES-MDR}Implement of GMRES-MDR}

The GMRES-MDR is almost the same as GCRO-DR, except for $3$ things.

\begin{enumerate}
  \item It set a threshold on eigen-values to decrease $k$ if a larger $k$ is not necessary.
  \item It check the speed of convergence to switch between REV and SVD.
  \item At first iteration, if $U_k$ is defined, it use another algorithm to obtain $U_k$ and $C_k$.
\end{enumerate}

\begin{algorithm}[H]
\begin{algorithmic}
\State {$[Q,R]=U_k$}
\If {REV}
\State {$Q^{\dagger}AQ \omega = \theta \omega$}
\EndIf
\If {HEV}
\State {$Q^{\dagger}A^{\dagger}AQ \omega = \theta Q^{\dagger}A^{\dagger}Q\omega$}
\EndIf
\If {SVD}
\State {$Q^{\dagger}A^{\dagger}AQ \omega = \theta^2 \omega$}
\EndIf
\State {$U_k=Q\omega _k$}
\end{algorithmic}
\caption{First iteration of GMRES-MDR if $U_k$ is defined.}
\end{algorithm}

We only implement the third because the first two can be tunable by parameters.

\subsubsection{\label{sec:testofgcrodr}Test of GCRO-DR and GMRES-MDR}

It is tested that, for both GCRO-DR and GMRES-MDR are suitable for the low-mode case.

We run with unitary gauge and $\kappa = 0.1249$. ($\kappa_c=0.125$). GCRO-DR with $m=16$ and $k=4$ will run even faster than $dim=50$ GMRES.

\textbf{However, if it is not the low-mode case, GMRES is faster.}

